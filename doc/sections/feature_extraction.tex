\section{Signal Processing}
\label{sec:processing}

This chapter outlines the signal processing techniques applied in this project. The primary objective is to extract meaningful features from audio signals to train a machine learning model effectively.

\subsection{Waveform}
The first step in feature extraction is analyzing the raw audio data. Before applying any signal processing, it is essential to inspect the data visually and audibly to identify abnormalities and determine suitable feature extraction strategies. The audio files are stored in MP3 format, with durations ranging from approximately four to ten seconds, sampled at 32 kHz. Playing back the audio files and visualizing their waveforms helps detect issues such as clipping or excessive noise.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\textwidth]{images/audio_signal.png}
    \caption{Visualization of the audio waveform.}
    \label{fig:audiosignal}
\end{figure}

Figure~\ref{fig:audiosignal} shows a typical audio waveform. The signal appears clean, without any visible clipping or excessive noise. While inspecting all audio files for abnormalities or conducting a statistical analysis would ensure data quality, this is beyond the scope of the project. Instead, it is assumed that the dataset is generally clean. However, silent segments of the audio are trimmed by applying a threshold of 30 dB, resulting in a cleaner signal shown in Figure~\ref{fig:audiosignaltrimmed}.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\textwidth]{images/audio_signal_trimmed.png}
    \caption{Visualization of the trimmed audio waveform.}
    \label{fig:audiosignaltrimmed}
\end{figure}

\subsection{Spectrogram}
Raw waveforms are not directly suitable for gender classification. Therefore, the audio signals are transformed into spectrograms, which provide a visual representation of frequency content over time. Spectrograms are generated using the Short-Time Fourier Transform (STFT). This produces a two-dimensional matrix where the x-axis represents time, the y-axis represents frequency, and the color intensity represents the power of the signal at each time and frequency.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\textwidth]{images/spectrogram.png}
    \caption{Visualization of the spectrogram.}
    \label{fig:spectrogram}
\end{figure}

The spectrogram is computed as:
\[
X[n, k] = \sum_{m=0}^{M-1} x[m] \cdot w[m-n] \cdot e^{-j2\pi km/M}
\]
where \(x[m]\) is the signal, \(w[m-n]\) is the window function, and \(M\) is the frame length.

\subsection{Mel-Spectrogram}
To better capture human auditory perception, the spectrogram is transformed into a mel-spectrogram. The mel scale is a perceptual frequency scale that closely aligns with how humans perceive sound. A mel filter bank is applied to the spectrogram, producing a two-dimensional matrix where the x-axis represents time and the y-axis represents mel frequencies.

\[
S_{\text{mel}}[m, n] = \sum_{k=1}^{K} H[m, k] \cdot |S[k, n]|^2
\]
where:
\begin{align*}
S[k, n] & = \text{Short-Time Fourier Transform (STFT) of the signal} \\
H[m, k] & = \text{mel filter bank with } m \text{ filters and } k \text{ frequency bins.}
\end{align*}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\textwidth]{images/mel-spectrogram.png}
    \caption{Visualization of the mel-spectrogram.}
    \label{fig:mel-spectrogram}
\end{figure}

\subsection{Resizing}
The mel-spectrogram must be resized to a fixed dimension for input into the machine learning model. This project uses bilinear interpolation to resize the mel-spectrogram to \(64 \times 64\) pixels. Resizing reduces the input data's complexity, facilitating efficient model training. The resized mel-spectrogram is shown in Figure~\ref{fig:resized-mel-spectrogram}.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\textwidth]{images/dataset_samples.png}
    \caption{Visualization of the resized mel-spectrogram.}
    \label{fig:resized-mel-spectrogram}
\end{figure}
