{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gender Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import librosa \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sounddevice as sd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchmetrics\n",
    "import torch.optim.adam\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from scipy.io.wavfile import write\n",
    "\n",
    "from IPython.display import Audio\n",
    "\n",
    "# import date time\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hardware for training\n",
    "\n",
    "the following lines determine on what hardware the neural network is gonne be trained on. Luckily there is some hardware available which should enable faster training due to usage of GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "print(\"CUDA version:\", torch.version.cuda)\n",
    "print(\"GPU Name:\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"No GPU detected\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This are some Constant which are used for accessing data from the f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_PATH = '../doc/images/' # path to save images for LaTeX import\n",
    "AUDIO_PATH = 'data/cv-corpus-16.1-delta-2023-12-06/en/clips/' # path to audio files\n",
    "LABEL_PATH = 'data/cv-corpus-16.1-delta-2023-12-06/en/validated.tsv' # path to labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function saves a table from for example a dataframe to a file in Latex format, so it can be imported easily"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def table_to_latex(df: pd.DataFrame, filename: str) -> None:\n",
    "    \"\"\"\n",
    "    Exports a pandas DataFrame to a LaTeX file.\n",
    "\n",
    "    Parameters:\n",
    "    - df (pd.DataFrame): The DataFrame to export.\n",
    "    - filename (str): The name of the file (without extension) where the LaTeX table will be saved.\n",
    "    \"\"\"\n",
    "    # Generate LaTeX code from the DataFrame\n",
    "    latex_code = df.to_latex(index=False, escape=True)\n",
    "\n",
    "    \n",
    "    with open(f\"../doc/tables/{filename}.tex\", \"w\") as f:\n",
    "        f.write(latex_code)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Analyses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = pd.read_csv(LABEL_PATH, sep='\\t', header=0,) # load labels from tsv file\n",
    "labels.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop unecesary columns\n",
    "labels = labels.drop(columns=['client_id',\n",
    "                              'up_votes', \n",
    "                              'down_votes', \n",
    "                              'accents', \n",
    "                              'locale', \n",
    "                              'segment', \n",
    "                              'variant', \n",
    "                              'sentence',])\n",
    "\n",
    "# drop rows with missing values\n",
    "labels = labels.dropna(subset=['gender'])\n",
    "print(labels['gender'].value_counts())\n",
    "\n",
    "# Create the bar plot\n",
    "gender_feature_plot = labels['gender'].value_counts().plot(kind='bar', figsize=(10, 5))\n",
    "gender_feature_plot.set_title('Gender Features')  # Set title for the plot\n",
    "gender_feature_plot.set_xlabel('Gender')          # Set x-axis label\n",
    "gender_feature_plot.set_ylabel('Count')           # Set y-axis label\n",
    "\n",
    "experiment_labels = labels[labels['gender'].isin(['other'])]\n",
    "\n",
    "# Filter the DataFrame to keep only rows where 'gender' is 'male' or 'female'\n",
    "labels = labels[labels['gender'].isin(['male', 'female'])]\n",
    "\n",
    "\n",
    "# replace the string values with integers\n",
    "labels['gender'] = labels['gender'].replace({'male': 0, 'female': 1}).astype(int)\n",
    "\n",
    "# Count the number of samples in each class\n",
    "male_count = labels[labels['gender'] == 0].shape[0]\n",
    "female_count = labels[labels['gender'] == 1].shape[0]\n",
    "\n",
    "# Determine the minority class count\n",
    "min_count = min(male_count, female_count)\n",
    "\n",
    "# Randomly sample the majority class\n",
    "balanced_labels = pd.concat([\n",
    "    labels[labels['gender'] == 0].sample(n=min_count, random_state=42),\n",
    "    labels[labels['gender'] == 1].sample(n=min_count, random_state=42)\n",
    "])\n",
    "\n",
    "print(balanced_labels['gender'].value_counts())\n",
    "\n",
    "labels = balanced_labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the figure\n",
    "fig = gender_feature_plot.get_figure()\n",
    "fig.savefig(IMAGE_PATH + 'gender_features.png', dpi=600, bbox_inches='tight', transparent=True)\n",
    "table_to_latex(labels.head(), 'features')\n",
    "\n",
    "# Create a figure and axis\n",
    "fig, ax = plt.subplots(figsize=(16, 3))  # Adjust figsize for better image quality\n",
    "ax.axis('tight')\n",
    "ax.axis('off')\n",
    "\n",
    "# Render the table\n",
    "table = ax.table(cellText=labels.head().values, colLabels=labels.head().columns, loc='center')\n",
    "# Adjust font size\n",
    "table.auto_set_font_size(False) \n",
    "table.set_fontsize(15) \n",
    "table.scale(1, 2)  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Signal Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Audio output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def listen(paths, path_idx=0):\n",
    "    # Update paths with the base directory path\n",
    "    paths = [os.path.join(AUDIO_PATH, path) for path in labels['path'].values]\n",
    "\n",
    "    # Load the audio file (example: first file in paths list)\n",
    "    audio_data, sample_rate = librosa.load(paths[path_idx], sr=None)  # `sr=None` keeps the original sample rate\n",
    "    # print(f\"Number of samples: {len(audio_data)}, sample rate: {sample_rate}\")\n",
    "    Audio(audio_data, rate=sample_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = [os.path.join(AUDIO_PATH, path) for path in labels['path'].values]\n",
    "for path in paths[:10]:\n",
    "    audio_data, sample_rate = librosa.load(path, sr=None)\n",
    "    #audio_data = librosa.effects.trim(audio_data, top_db=30)[0]  # Trim silent edges\n",
    "    print(f\"Number of samples: {len(audio_data)}, sample rate: {sample_rate}, Time: {len(audio_data)/sample_rate:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Waveform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_data, sample_rate = librosa.load(paths[2], sr=None)\n",
    "\n",
    "title = 'Audio Signal'\n",
    "t = np.linspace(0, len(audio_data)/sample_rate, len(audio_data))\n",
    "plt.figure(figsize=(15, 5))\n",
    "plt.plot(t, audio_data)\n",
    "plt.xlabel('Time [s]')\n",
    "plt.ylabel('Amplitude')\n",
    "plt.title(title)\n",
    "plt.grid()\n",
    "plt.savefig(IMAGE_PATH + 'audio_signal.png', dpi=600, bbox_inches='tight', transparent=True)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Trim leading and trailing silence from an audio signal\n",
    "trimmed_audio = librosa.effects.trim(audio_data, top_db=30)[0]\n",
    "t = np.linspace(0, len(trimmed_audio)/sample_rate, len(trimmed_audio))\n",
    "\n",
    "title = 'Audio Signal (Trimmed)'\n",
    "plt.figure(figsize=(15, 5))\n",
    "plt.plot(t, trimmed_audio)\n",
    "plt.xlabel('Time [s]')\n",
    "plt.ylabel('Amplitude')\n",
    "plt.title(title)\n",
    "plt.grid()\n",
    "plt.savefig(IMAGE_PATH + 'audio_signal_trimmed.png', dpi=600, bbox_inches='tight', transparent=True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spectrogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 1024  # FFT window size\n",
    "hop_length = 256  # Overlap between frames, adjust as needed\n",
    "window = 'hann'  # Window type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Sx = librosa.stft(trimmed_audio, n_fft=N, hop_length=hop_length, window=window)    # Calculate STFT\n",
    "Sx_dB = librosa.amplitude_to_db(abs(Sx), ref=np.max)    # spectrogram to dB scale\n",
    "\n",
    "# Plot the spectrogram\n",
    "title = f'Spectrogram {Sx_dB.shape}'\n",
    "fig, ax = plt.subplots(figsize=(15, 5))\n",
    "img = librosa.display.specshow(Sx_dB, sr=sample_rate, hop_length=hop_length, x_axis='time', y_axis='hz', cmap='viridis')\n",
    "ax.set_title(title)\n",
    "fig.colorbar(img, ax=ax, format=\"%+2.0f dB\", label='Intensity [dB]')\n",
    "plt.xlabel(\"Time [s]\")\n",
    "plt.ylabel(\"Frequency [Hz]\")\n",
    "fig.savefig(IMAGE_PATH + 'spectrogram' + '.png', dpi=600, bbox_inches='tight', transparent=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_frames = (audio_data.shape[0])/hop_length + 1\n",
    "time_frames\n",
    "frequncy_bins = N//2 + 1\n",
    "time_frames, frequncy_bins"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Melspectrogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Sx_mel = librosa.feature.melspectrogram(y=trimmed_audio, sr=sample_rate, n_fft=N, hop_length=hop_length)\n",
    "Sx_mel_dB = librosa.power_to_db(Sx_mel, ref=np.max)\n",
    "\n",
    "# Plot the mel spectrogram\n",
    "title = f'Mel Spectrogram {Sx_mel_dB.shape} '\n",
    "fig, ax = plt.subplots(figsize=(15, 5))\n",
    "img = librosa.display.specshow(Sx_mel_dB, sr=sample_rate, hop_length=hop_length, x_axis='time', y_axis='mel', cmap='viridis')\n",
    "ax.set_title(title)\n",
    "fig.colorbar(img, ax=ax, format=\"%+2.0f dB\", label='Intensity [dB]')\n",
    "plt.xlabel(\"Time [s]\")\n",
    "plt.ylabel(\"Frequency [Hz]\")\n",
    "fig.savefig(IMAGE_PATH + 'mel-spectrogram' + '.png', dpi=600, bbox_inches='tight', transparent=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import librosa\n",
    "import numpy as np\n",
    "\n",
    "def extract_mel_spectrogram(audio, sr=22050, n_fft=1024, hop_length=512, n_mels=64, target_shape=(64, 64)):\n",
    "    \"\"\"\n",
    "    Extract a Mel spectrogram from an audio signal and resize it to the target shape.\n",
    "    \n",
    "    Parameters:\n",
    "    - audio: 1D NumPy array containing the audio signal.\n",
    "    - sr: Sampling rate (default 22050).\n",
    "    - n_fft: Length of the FFT window (default 1024).\n",
    "    - hop_length: Number of samples between successive frames (default 512).\n",
    "    - n_mels: Number of Mel bands (default 64).\n",
    "    - target_shape: Desired output shape (default (64, 64)).\n",
    "\n",
    "    Returns:\n",
    "    - mel_spectrogram: PyTorch tensor of shape [1, 1, target_shape[0], target_shape[1]].\n",
    "    \"\"\"\n",
    "    # Trim leading and trailing silence\n",
    "    audio = librosa.effects.trim(audio, top_db=30)[0]\n",
    "    \n",
    "    # Compute the Mel spectrogram\n",
    "    mel_spectrogram = librosa.feature.melspectrogram(\n",
    "        y=audio,\n",
    "        sr=sr,\n",
    "        n_fft=n_fft,\n",
    "        hop_length=hop_length,\n",
    "        n_mels=n_mels\n",
    "    )\n",
    "    \n",
    "    # Convert to decibel scale\n",
    "    mel_spectrogram = librosa.power_to_db(mel_spectrogram, ref=np.max)\n",
    "    \n",
    "    # Convert to PyTorch tensor\n",
    "    mel_spectrogram = torch.tensor(mel_spectrogram, dtype=torch.float32)\n",
    "    \n",
    "    # Add channel and batch dimensions for resizing\n",
    "    mel_spectrogram = mel_spectrogram.unsqueeze(0).unsqueeze(0)  # Shape: [1, 1, H, W]\n",
    "    \n",
    "    # Resize to the target shape\n",
    "    mel_spectrogram = F.interpolate(mel_spectrogram, size=target_shape, mode=\"bilinear\", align_corners=False)\n",
    "    \n",
    "    return mel_spectrogram\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import librosa\n",
    "\n",
    "class AudioDataset(Dataset):\n",
    "    def __init__(self, audio_labels, audio_dir, transform=None):\n",
    "        \"\"\"\n",
    "        A PyTorch Dataset for loading audio files and their labels.\n",
    "\n",
    "        Args:\n",
    "        - audio_labels (pd.DataFrame): A DataFrame containing audio file names and labels.\n",
    "        - audio_dir (str): Path to the directory containing audio files.\n",
    "        - transform (callable, optional): A function/transform to apply to the spectrogram.\n",
    "        \"\"\"\n",
    "        self.labels = audio_labels\n",
    "        self.dir = audio_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Return the number of samples in the dataset.\n",
    "        \"\"\"\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Load an audio sample and its corresponding label.\n",
    "        \n",
    "        Args:\n",
    "        - idx (int): Index of the sample.\n",
    "\n",
    "        Returns:\n",
    "        - spec (torch.Tensor): Mel spectrogram of the audio sample (default shape: [64, 64]).\n",
    "        - label (torch.Tensor): Corresponding label as a tensor (shape: [1]).\n",
    "        \"\"\"\n",
    "        # Load audio file\n",
    "        audio_path = os.path.join(self.dir, self.labels.iloc[idx, 0])\n",
    "        if not os.path.exists(audio_path):\n",
    "            raise FileNotFoundError(f\"Audio file not found: {audio_path}\")\n",
    "        \n",
    "        audio, _ = librosa.load(audio_path, sr=None)  # Load audio with original sampling rate\n",
    "\n",
    "        # Generate Mel spectrogram\n",
    "        spec = extract_mel_spectrogram(audio)  # Default shape: [64, 64]\n",
    "\n",
    "        # Apply optional transformation to the spectrogram\n",
    "        if self.transform:\n",
    "            spec = self.transform(spec)\n",
    "        \n",
    "        # Retrieve label and convert to tensor\n",
    "        label = self.labels.iloc[idx, 2]\n",
    "        label = torch.tensor(label, dtype=torch.long).unsqueeze(0)  # Shape: [1]\n",
    "\n",
    "        return spec, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = AudioDataset(labels, AUDIO_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot 4 random samples from the dataset\n",
    "fig, axs = plt.subplots(1,4, figsize=(20, 10))\n",
    "for i in range(4):\n",
    "    spec, label = data[i]\n",
    "    ax = axs[i]\n",
    "    ax.imshow(spec.squeeze(0).squeeze(0), cmap='viridis', origin='lower')\n",
    "    ax.set_title(f'Label: {label}, Shape: {spec.shape}')\n",
    "    ax.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.savefig(IMAGE_PATH + 'dataset_samples.png', dpi=600, bbox_inches='tight', transparent=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GenderClassifier2D(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(GenderClassifier2D, self).__init__()\n",
    "        \n",
    "        # Convolutional layers\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=48, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv3 = nn.Conv2d(in_channels=48, out_channels=120, kernel_size=3, stride=1, padding=1)\n",
    "        \n",
    "        # Pooling layer\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(7680, 128)  # Adjusted based on conv output size\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, 2)  # Output layer with 2 units for binary classification\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Convolutional and pooling layers\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.pool(x)\n",
    "        \n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.pool(x)\n",
    "        \n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = self.pool(x)\n",
    "        \n",
    "        # Flatten for the fully connected layers\n",
    "        x = x.view(x.size(0), -1)  # Flatten to [batch_size, features]\n",
    "        \n",
    "        # Fully connected layers\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)  # No activation here because CrossEntropyLoss will handle it\n",
    "        \n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GenderClassifier2D().to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and Validation Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, val_data, test_data = torch.utils.data.random_split(data, [0.8, 0.15, 0.05], generator=torch.Generator().manual_seed(42))\n",
    "len(data), len(train_data), len(val_data), len(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss function and Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-4\n",
    "batch_size = 521\n",
    "epochs = 1\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = SummaryWriter(\"log/test\")\n",
    "writer.add_graph(model, torch.rand(1, 1, 64, 64).to(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision = torchmetrics.Precision(num_classes=2, task=\"binary\")\n",
    "recall = torchmetrics.Recall(num_classes=2, task='binary')\n",
    "f1_score = torchmetrics.F1Score(num_classes=2, task='binary')\n",
    "accuracy_metric = torchmetrics.Accuracy(task='binary')\n",
    "\n",
    "def test_loop(dataloader, model, loss_fn, writer=None, epoch=None):\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    size = len(dataloader.dataset)  # Total number of samples\n",
    "    test_loss = 0.0\n",
    "\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    # Reset metrics before starting evaluation\n",
    "    precision.reset()\n",
    "    recall.reset()\n",
    "    f1_score.reset()\n",
    "    accuracy_metric.reset()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            # Move data to the appropriate device\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            pred = model(X)\n",
    "            \n",
    "            # Compute loss\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            \n",
    "            # Collect predictions and labels for metric calculation\n",
    "            all_preds.append(pred.argmax(1).cpu())\n",
    "            all_labels.append(y.cpu())\n",
    "\n",
    "    # Calculate metrics\n",
    "    all_preds = torch.cat(all_preds)\n",
    "    all_labels = torch.cat(all_labels)\n",
    "\n",
    "    precision_score = precision(all_preds, all_labels).item()\n",
    "    recall_score = recall(all_preds, all_labels).item()\n",
    "    f1 = f1_score(all_preds, all_labels).item()\n",
    "    accuracy_score = accuracy_metric(all_preds, all_labels).item()\n",
    "\n",
    "    # Calculate average loss\n",
    "    test_loss /= len(dataloader)\n",
    "\n",
    "    print(f\"Test Error: \\n Accuracy: {accuracy_score * 100:.2f}%, Avg loss: {test_loss:.6f}\")\n",
    "    print(f\"Precision: {precision_score:.4f}, Recall: {recall_score:.4f}, F1 Score: {f1:.4f}\")\n",
    "\n",
    "    # Log metrics to TensorBoard if writer is provided\n",
    "    if writer and epoch is not None:\n",
    "        writer.add_scalar('Loss/Test', test_loss, epoch)\n",
    "        writer.add_scalar('Accuracy/Test', accuracy_score, epoch)\n",
    "        writer.add_scalar('Precision/Test', precision_score, epoch)\n",
    "        writer.add_scalar('Recall/Test', recall_score, epoch)\n",
    "        writer.add_scalar('F1 Score/Test', f1, epoch)\n",
    "\n",
    "    return test_loss, accuracy_score, precision_score, recall_score, f1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(epochs):  # loop over the dataset multiple times\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for i, data in enumerate(train_data, 0):\n",
    "        # basic training loop\n",
    "        inputs, labels = data\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        if i % batch_size == batch_size-1:\n",
    "            \n",
    "            # Check against the validation set\n",
    "            running_vloss = 0.0\n",
    "\n",
    "            # In evaluation mode some model specific operations can be omitted eg. dropout layer\n",
    "            model.train(False) # Switching to evaluation mode, eg. turning off regularisation\n",
    "            for j, vdata in enumerate(val_data, 0):\n",
    "                vinputs, vlabels = vdata\n",
    "                vinputs, vlabels = vinputs.to(device), vlabels.to(device)\n",
    "                voutputs = model(vinputs)\n",
    "                vloss = criterion(voutputs, vlabels)\n",
    "                running_vloss += vloss.item()\n",
    "            model.train(True) # Switching back to training mode, eg. turning on regularisation\n",
    "\n",
    "            avg_loss = running_loss / batch_size\n",
    "            avg_vloss = running_vloss / len(val_data)\n",
    "\n",
    "            # Log the running loss averaged per batch\n",
    "            writer.add_scalars('Training vs. Validation Loss',\n",
    "                            { 'Training' : avg_loss, 'Validation' : avg_vloss },\n",
    "                            epoch * len(train_data) + i)\n",
    "            print('Batch {}'.format(i + 1) + ' loss: {}'.format(avg_loss) + ' validation loss: {}'.format(avg_vloss))\n",
    "            running_loss = 0.0\n",
    "    # save the model with epoch number and timestamp\n",
    "    torch.save(model.state_dict(), f\"models/model_{epoch}_{datetime.now().strftime('%Y%m%d%H%M')}.pth\")\n",
    "\n",
    "    # Testing the model on the test data set\n",
    "\n",
    "    test_loop(test_data, model, criterion)\n",
    "    \n",
    "    print('Finished Training')\n",
    "\n",
    "writer.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loop(test_data, model, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(epochs):  # Loop over the dataset multiple times\n",
    "    running_loss = 0.0\n",
    "    model.train()  # Enable training mode\n",
    "\n",
    "    for i, data in enumerate(train_data):\n",
    "        inputs, labels = data\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward + backward + optimize\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "\n",
    "        # Accumulate batch loss\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        # Log and reset losses every logging interval\n",
    "        if (i + 1) % log_interval == 0:\n",
    "            avg_loss = running_loss / log_interval\n",
    "            running_loss = 0.0\n",
    "\n",
    "            # Run validation\n",
    "            model.eval()  # Disable dropout, batchnorm updates\n",
    "            val_loss = 0.0\n",
    "            with torch.no_grad():\n",
    "                for vinputs, vlabels in val_data:\n",
    "                    vinputs, vlabels = vinputs.to(device), vlabels.to(device)\n",
    "                    voutputs = model(vinputs)\n",
    "                    vloss = criterion(voutputs, vlabels)\n",
    "                    val_loss += vloss.item()\n",
    "\n",
    "            avg_vloss = val_loss / len(val_data)\n",
    "\n",
    "            # Log metrics to TensorBoard\n",
    "            writer.add_scalars(\n",
    "                'Training vs. Validation Loss',\n",
    "                {'Training': avg_loss, 'Validation': avg_vloss},\n",
    "                epoch * len(train_data) + i,\n",
    "            )\n",
    "\n",
    "            print(f\"Batch {i + 1}: Training Loss = {avg_loss:.4f}, Validation Loss = {avg_vloss:.4f}\")\n",
    "            model.train()  # Re-enable training mode\n",
    "\n",
    "    # Save the model with epoch number and timestamp\n",
    "    torch.save(model.state_dict(), f\"models/model_{epoch}_{datetime.now().strftime('%Y%m%d%H%M')}.pth\")\n",
    "\n",
    "    # Test the model on the test data\n",
    "    test_loop(test_data, model, criterion)\n",
    "\n",
    "    print(f\"Epoch {epoch + 1} finished.\\n\")\n",
    "\n",
    "writer.flush()\n",
    "writer.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# experiment_labels\n",
    "paths = [os.path.join(AUDIO_PATH, path) for path in experiment_labels['path'].values]\n",
    "\n",
    "\n",
    "# Load the audio file (example: first file in paths list)\n",
    "\n",
    "audio_data, sample_rate = librosa.load(paths[1], sr=None)  # `sr=None` keeps the original sample rate\n",
    "# print(f\"Number of samples: {len(audio_data)}, sample rate: {sample_rate}\")\n",
    "Audio(audio_data, rate=sample_rate)\n",
    "len(paths)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_gender(audio_data, model):\n",
    "    model.eval()\n",
    "\n",
    "    # Preprocess the input\n",
    "    processed_input = extract_mel_spectrogram(audio_data).to(device)\n",
    "    # processed_input = processed_input.unsqueeze(0).unsqueeze(0)  # Add batch and channel dimensions\n",
    "    # print(processed_input.shape)\n",
    "    # Make prediction\n",
    "    with torch.no_grad():\n",
    "        output = model(processed_input)\n",
    "        probabilities = torch.softmax(output, dim=1)\n",
    "        predicted_class = torch.argmax(probabilities).item()\n",
    "    \n",
    "    return predicted_class, probabilities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the best model\n",
    "# model_string = 'models/test.pth'\n",
    "# Use weights_only=True for secure loading\n",
    "map_location = torch.device('cpu')  # Force the model to load on CPU\n",
    "state_dict = torch.load(model_string, map_location=map_location)  # Map tensors to CPU\n",
    "model.load_state_dict(state_dict)\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio = librosa.load(paths[39], sr=None)[0]\n",
    "\n",
    "\n",
    "classifier = predict_gender(audio, model)\n",
    "if classifier[0] == 0:\n",
    "    print('Male with probability:', classifier[1][0][0].item())\n",
    "else:\n",
    "    print('Female with probability:', classifier[1][0][1].item())\n",
    "print()\n",
    "Audio(audio, rate=sample_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loop(test_data, model, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# voice_path = 'data/pete.wav'\n",
    "# voice_path = 'data/tobi.wav'\n",
    "# voice_path = 'data/chalisa.wav'\n",
    "# voice_path = 'data/chalisa2.wav'\n",
    "\n",
    "audio, sample_rate = librosa.load(voice_path, sr=None)\n",
    "# print(f\"Number of samples: {len(audio)}, sample rate: {sample_rate}\")\n",
    "\n",
    "\n",
    "# downsample to 32kHz\n",
    "audio = librosa.resample(audio, orig_sr=sample_rate, target_sr=32000)\n",
    "sample_rate = 32000\n",
    "\n",
    "\n",
    "classifier = predict_gender(audio, model)\n",
    "if classifier[0] == 0:\n",
    "    print('Male with probability:', classifier[1][0][0].item())\n",
    "else:\n",
    "    print('Female with probability:', classifier[1][0][1].item())\n",
    "print()\n",
    "Audio(audio, rate=sample_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def record_audio(filename='recording.wav', duration=5, sample_rate=48000, device_index=2, channels=2):\n",
    "    \"\"\"\n",
    "    Record audio and save to a .wav file.\n",
    "    \n",
    "    Parameters:\n",
    "    - filename: Name of the output file.\n",
    "    - duration: Duration of the recording in seconds.\n",
    "    - sample_rate: Sample rate in Hz.\n",
    "    - device_index: Index of the capture device.\n",
    "    - channels: Number of audio channels (1 for mono, 2 for stereo).\n",
    "    \"\"\"\n",
    "    print(\"Recording...\")\n",
    "    # Record audio with the specified number of channels\n",
    "    audio_data = sd.rec(int(duration * sample_rate), samplerate=sample_rate, channels=channels, device=device_index, dtype='float32')\n",
    "    sd.wait()  # Wait until the recording is finished\n",
    "    print(\"Recording complete.\")\n",
    "\n",
    "    # Save as a .wav file\n",
    "    write(filename, sample_rate, audio_data)\n",
    "    print(f\"Audio saved as {filename}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the saved model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "record_audio('recording/test.wav', duration=2, device_index=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import soundfile as sf\n",
    "audio_data, sample_rate = librosa.load('test.wav', sr=None)\n",
    "print(f\"Number of samples: {len(audio_data)}, sample rate: {sample_rate}\")\n",
    "\n",
    "# plot the audio signal\n",
    "t = np.linspace(0, len(audio_data)/sample_rate, len(audio_data))\n",
    "plt.figure(figsize=(15, 5))\n",
    "plt.plot(t, audio_data)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
